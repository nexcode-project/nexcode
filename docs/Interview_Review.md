# 复盘

## 模型评测
大模型的核心工作是数据的建设，高质量的数据对于模型性能有着决定性的作用，其他的模型架构只是用来实现模型推理的高效性

### 数据集建设
模型测试以及训练数据的质量决定着模型解决问题的能力判断与模型能力的提升，在这方面，我利用公司内部高质量仓库以及github上活跃度，star数较多的仓库建立模型评测与训练数据集。
### 数据闭环建设-用户反馈评测
在MiCopilot和CodeReview项目中，为了实现模型能力的持续提升，我设计了一套数据闭环系统以支持持续不断从用户侧获取反馈，获取到模型真实的人类评测结果。具体做法如下：
#### 在MiCopilot 系统中
简要：通过介入到前端用户操作，获取用户最终的代码数据与模型生成数据，生成正负样本对。
* 首先在补全场景下，模型会根据上下文生成相对应的补全代码。
* 用户可接受与拒绝模型生成的代码
    * 接受的情况下，仍然需要进行延迟判断，用户是否对接受的代码进行了撤回等操作
    * 撤回， 负样本。
    * 接受且未修改，正样本。
    * 接受且修改部分内容，中性样本。
    * 拒绝的情况下，负样本。   

#### 在CodeReview系统中
简要：用户可以生成的CodeReview信息进行判断，判断出其中正确的Review结果并通过按钮进行反馈
* 首先，大模型会根据MR信息生成相对应的多条CodeReview信息。
* 用户可接受和拒绝模型生成的代码
    * 接受的情况下，正样本
    * 拒绝的情况下，负样本

## 模型自动化评测系统

### 实现

实现模型自动化评测，实现定时评测与一键评测。
OpenCompass和SWE-Bench

#### OpenCompass的实现思路
简要：通过对数据集与模型接口的抽象，实现通过配置文件读取要评测的数据集与模型
* 首先，对数据集进行抽象，合成出数据集的System Prompt和User Prompt
* 对各方模型入口进行定义，OpenAI Compatible
* 对结果进行计算，得出评测结果 EM（Exact Match） 和 BLEU（Bilingual Evaluation Understudy）

#### SWE-Bench的实现思路
简要：通过Docker创建模型的评测环境，实现模型生成补丁文件，通过容器化环境进行执行评测
* 获取高质量的仓库
* 爬取其所有的PR数据
* 创建相对应的环境，相同的仓库共用相同的base镜像。而每个PR有独特的各自的镜像。
* 首先，是数据集的建设，合成出数据集的System Prompt和User Prompt。
* 根据仓库与Commit 创建相应的容器。
* 将patch应用在容器中，运行容器内自动化测试。

## 模型训练

### 模型Lora

#### 主要参数
**通用**
* Learning Rate
* Batch Size
* Number of Epochs
**Lora**
* Rank r=8
* Alpha  = 16
* Dropout = 0.1
**P-tuning**
* num_virtual_tokens=20

#### 目标模块
['q_proj', 'v_proj']

#### 为什么不用RLHF

这就要提到LoRA和RLHF之间的区别

* LoRA：
    * 优点：简单稳定、收敛块、资源需求低
    * 缺点：效果天花板较低、偏好学习能力弱
* RLHF：
    * 优点：效果上限高
    * 缺点：极其复杂、不稳定、资源消耗大

项目初期，采用LoRA进行训练，快速让基座模型适应代码指令格式，成本低、见效快。
